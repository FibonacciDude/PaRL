
Have some simple logging:
	error
	multiprocessing
	get the rewards


multiprocessing:
	shared tensor -> when sync:
			stop updates.
			-------
			zero out shared tensor
			each add to shared tensor * 1/10
			-------
			make all local tensors the shared one
	*Figure out: how to syncronize all processes without stopping everything
		-maybe a counter variable they all follow.
	each learns individually - normal algorithm with one agent (can be which ever)
		-we can even try shared V value (or different)
			-> what would it require a shared replay buffer?

	MuZero has latest storage of the latest network, some normalization values, etc

	I'll try EASGD for now as the averaging scheme seems somewhat unstable.

	What should be used for gradient descent (delta) given model theta.


	
	Algorithm:
		-if cc:
			-stop your local processes until further notice
			-for each process, they all add 
			*(instead of adds, you can put global changes to a queue, and pop in master)
			alpha * (x-x_bar) = v to the shared "upd_tensor" [WRITE]

			-after the process adds, it changes local to subtract v

			-once they all finish (?), 
			*(with the possible addition of summing all queue pops)
			the shared param is updated by master to x + "upd_tensor"
			-zero out "upd_tensor" by master
			-processes can continue ("further noticed")
		-else:
			non-distributed rl (subalgorithm - baseline)
			env:
			-store local replays using policy - pi (multiple times for batched?)
			rl:
			-update pi with replays using favorite algo
			-update q/v functions
			*(q/v functions can be updated with shared replay buffer for offline)
				-but no. It increases overhead (to every single update) and it can be 					solved with just a higher batchsize

